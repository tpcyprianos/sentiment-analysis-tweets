{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Valência em Tweets\n",
    "Este Notebook contém os resultados de uma das etapas do Projeto Final da disciplina de \"IA369-Y\" na UNICAMP. São apresentados aqui os resultados da implementação de um classificador de valência (1 a 7) em tweets utilizando uma abordagem de aprendizado de máquina, especificamente, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "O conjunto de dados utilizados nesta implementação são referentes aos tweets importados pela API [Tweepy](http://www.tweepy.org/) e pré-processados. O dataset é composto com as seguintes colunas:\n",
    "\n",
    "1. id - Usuário no Twitter\n",
    "2. txt - O conteúdo do tweet\n",
    "3. val - Valência de 1 a 7\n",
    "4. int - Intensidade de 1 a 7\n",
    "5. cit - Cidade e País do usuário\n",
    "6. data - Data e hora do tweet\n",
    "7. dia - Dia da semana\n",
    "\n",
    "1318 amostras x 7 colunas\n",
    "\n",
    "Para o tratamento dos emojis, foram gerados datasets com diferentes tratamentos:\n",
    "\n",
    "1. **output-pleasure-arousal-labeled**- Os emojis presentes nos tweets são excluídos\n",
    "2. **output-pleasure-arousal-labeled-emoji** - Os emojis são mantidos, porém sem tratamento, foi observado que em uma das etapas eles são desconsiderados\n",
    "3. **demojized_emojios** - Os emojis são tratados pela biblioteca [Demojize](https://github.com/nkmrtty/demojize.py/blob/master/demojize.py), ou seja, são transcritos, por exemplo *:smirking_face:* e *:yellow_heart:*\n",
    "\n",
    "Ao final, o dataset selecionado foi o **demojized_emojios**, pois foi possível incluir os emojis na abordagem adotada pelo classificador. Entendemos que emojis, principalmente nos tweets, carregam informações valiosas sobre a valência e intensidade.\n",
    "\n",
    "## Linguagens e Bibliotecas\n",
    "\n",
    "1. Ambiente: [Anaconda3 4.3.1](https://repo.continuum.io/archive/index.html)\n",
    "2. Linguagem de Programação: [Python 3.3](https://www.python.org/) \n",
    "3. Biblioteca de Dataframe: [Panda 0.19.2](http://pandas.pydata.org/).\n",
    "4. Machine Learning: [Scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "5. Plotting: [Matplotlib](https://matplotlib.org/)\n",
    "\n",
    "## Abordagem\n",
    "O dataset foi pré-processado, foram extraídas *features* de texto, separados em dados de treinamento e teste, e por fim, implementado um algoritmo de *Supporte Vector Machines* (SVM), conforme detalhes abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "# encoding: iso-8859-1\n",
    "# encoding: win-1252\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "As etapas adotadas para pré-processar o dataset foram:\n",
    "\n",
    "1. Separação de Data e Hora\n",
    "2. Substituição das Cidades por Valores\n",
    "3. Criação de Features [É madrugada e É final de semana]\n",
    "4. Balanceamento das amostras\n",
    "5. Pré-processamento dos tweets\n",
    "6. Tokenizing\n",
    "7. Remoção das StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset com Demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Com Demogize\n",
    "datasetInFrame = pd.read_csv(\"demojized_emojios.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)\n",
    "### Colocando espaços entre os emojis\n",
    "sentencesEmojis = [str(sentence).lower().replace(\":\",\" \") for sentence in datasetInFrame[\"txt\"]]\n",
    "datasetInFrame[\"txt\"] = sentencesEmojis\n",
    "datasetInFrame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset com Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Com emoji\n",
    "#datasetInFrame = pd.read_csv(\"output-pleasure-arousal-labeled-emoji.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sem Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Sem emoji\n",
    "#datasetInFrame = pd.read_csv(\"output-pleasure-arousal-labeled.csv\", sep=\"|\", encoding=\"ISO-8859-1\",quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>data</th>\n",
       "      <th>dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/2017 0:17:49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/2017 20:47:35</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/2017 9:45:33</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/2017 1:24:55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/2017 0:56:21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4   \n",
       "3  line_fdl                                      tô meio aérea    4    2   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4   \n",
       "\n",
       "                      cit                 data  dia  \n",
       "0  Rio de Janeiro, Brasil   10/27/2017 0:17:49    6  \n",
       "1  Rio de Janeiro, Brasil  10/26/2017 20:47:35    5  \n",
       "2  Rio de Janeiro, Brasil   10/26/2017 9:45:33    5  \n",
       "3  Rio de Janeiro, Brasil   10/26/2017 1:24:55    5  \n",
       "4  Rio de Janeiro, Brasil   10/26/2017 0:56:21    5  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição das Classes do Dataset\n",
    "- Valência (val): 1 a 7\n",
    "- Intensidade (int): 1 a 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 49, 2: 189, 3: 316, 4: 225, 5: 413, 6: 106, 7: 20}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalValClasses = {}\n",
    "\n",
    "for i in range(1,8):\n",
    "    totalValClasses[i] = datasetInFrame[datasetInFrame.val == i][\"txt\"].count()\n",
    "totalValClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb381a5d3c8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x='val', data=datasetInFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intensidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 49, 2: 189, 3: 316, 4: 225, 5: 413, 6: 106, 7: 20}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalIntClasses = {}\n",
    "\n",
    "for i in range(1,8):\n",
    "    totalIntClasses[i] = datasetInFrame[datasetInFrame.val == i][\"txt\"].count()\n",
    "totalIntClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb381a5d3c8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x='int', data=datasetInFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de data e hora\n",
    "Para gerar *features* posteriormente, foi adotada a estratégia separar os dados de data e hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>20:47:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>9:45:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>1:24:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>0:56:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4   \n",
       "3  line_fdl                                      tô meio aérea    4    2   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4   \n",
       "\n",
       "                      cit  dia        date      time  \n",
       "0  Rio de Janeiro, Brasil    6  10/27/2017   0:17:49  \n",
       "1  Rio de Janeiro, Brasil    5  10/26/2017  20:47:35  \n",
       "2  Rio de Janeiro, Brasil    5  10/26/2017   9:45:33  \n",
       "3  Rio de Janeiro, Brasil    5  10/26/2017   1:24:55  \n",
       "4  Rio de Janeiro, Brasil    5  10/26/2017   0:56:21  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateTime = datasetInFrame[\"data\"].apply(lambda  x: x.split(' '))\n",
    "date = dateTime.apply(lambda x: x[0])\n",
    "time = dateTime.apply(lambda x: x[1])\n",
    "datasetInFrame[\"date\"] = date\n",
    "datasetInFrame[\"time\"] = time\n",
    "del datasetInFrame[\"data\"]\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituição de Valores Para Cidade\n",
    "1. Removendo o País;\n",
    "2. Substituindo: \n",
    "    * Rio de Janeiro = 1 \n",
    "    * São Paulo = 2\n",
    "\n",
    "Essa estratégia foi adotada para incluir a cidade como uma *feature* no classificador. Apesar desse processamento, essa coluna não foi utilizada na implementação do modelo apresentado aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>20:47:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>9:45:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>1:24:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>0:56:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4   \n",
       "3  line_fdl                                      tô meio aérea    4    2   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4   \n",
       "\n",
       "              cit  dia        date      time  \n",
       "0  Rio de Janeiro    6  10/27/2017   0:17:49  \n",
       "1  Rio de Janeiro    5  10/26/2017  20:47:35  \n",
       "2  Rio de Janeiro    5  10/26/2017   9:45:33  \n",
       "3  Rio de Janeiro    5  10/26/2017   1:24:55  \n",
       "4  Rio de Janeiro    5  10/26/2017   0:56:21  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = datasetInFrame[\"cit\"].apply(lambda x: x.split(','))\n",
    "datasetInFrame[\"cit\"] = city.apply(lambda x: x[0])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>20:47:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>9:45:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>1:24:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>0:56:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  cit  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3    1   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4    1   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4    1   \n",
       "3  line_fdl                                      tô meio aérea    4    2    1   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4    1   \n",
       "\n",
       "   dia        date      time  \n",
       "0    6  10/27/2017   0:17:49  \n",
       "1    5  10/26/2017  20:47:35  \n",
       "2    5  10/26/2017   9:45:33  \n",
       "3    5  10/26/2017   1:24:55  \n",
       "4    5  10/26/2017   0:56:21  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetInFrame[\"cit\"] = datasetInFrame[\"cit\"].apply(lambda x: 1 if x == \"Rio de Janeiro\" else 2)\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de  *features*\n",
    "\n",
    "Após o processamento, definimos como *features* interessantes a serem consideradas como Final de Semana e Madrugada. Intuitivamente, os tweets tendem a ser mais felizes nos finais de semana, mas será que tal característica é de fato relevante? Como uma tentativa de responder a essa questão, foram adicionadas duas colunas de:\n",
    "\n",
    "1. isWeekend? [dia 6 após às 19:00, dia 7 e 1 até 19:00 - 1 sim, 0 não\n",
    "2. isMad? [23:00 às 5:00] - 1 sim, 0 não\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>20:47:35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>9:45:33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>1:24:55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>0:56:21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  cit  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3    1   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4    1   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4    1   \n",
       "3  line_fdl                                      tô meio aérea    4    2    1   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4    1   \n",
       "\n",
       "   dia        date      time  isWeekend  \n",
       "0    6  10/27/2017   0:17:49        0.0  \n",
       "1    5  10/26/2017  20:47:35        0.0  \n",
       "2    5  10/26/2017   9:45:33        0.0  \n",
       "3    5  10/26/2017   1:24:55        0.0  \n",
       "4    5  10/26/2017   0:56:21        0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isWeekend(day,hour):\n",
    "    format = '%H:%M:%S'\n",
    "    hour = datetime.strptime(hour, format)\n",
    "    limitWeekend = datetime.strptime(\"19:00:00\", format)\n",
    "    if (day == 6) and (hour >= limitWeekend):\n",
    "        return 1\n",
    "    elif (day == 1) and (hour < limitWeekend):\n",
    "        return 1\n",
    "    elif day == 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "datasetInFrame['isWeekend'] = pd.Series(np.zeros(len(datasetInFrame)), index=datasetInFrame.index)\n",
    "\n",
    "for index, row in datasetInFrame.iterrows():\n",
    "    datasetInFrame.loc[index,\"isWeekend\"] = isWeekend(row[\"dia\"],row[\"time\"])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>guilherme só aparece aqui em casa p fazer barb...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>20:47:35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>amo cheiro de café pela manhã..  hot_beverage ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>9:45:33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô meio aérea</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>1:24:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô mega cansada nn sei que cançaso é esse, só ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/26/2017</td>\n",
       "      <td>0:56:21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  cit  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3    1   \n",
       "1  line_fdl  guilherme só aparece aqui em casa p fazer barb...    5    4    1   \n",
       "2  line_fdl  amo cheiro de café pela manhã..  hot_beverage ...    5    4    1   \n",
       "3  line_fdl                                      tô meio aérea    4    2    1   \n",
       "4  line_fdl  tô mega cansada nn sei que cançaso é esse, só ...    2    4    1   \n",
       "\n",
       "   dia        date      time  isWeekend  isMad  \n",
       "0    6  10/27/2017   0:17:49        0.0    1.0  \n",
       "1    5  10/26/2017  20:47:35        0.0    0.0  \n",
       "2    5  10/26/2017   9:45:33        0.0    0.0  \n",
       "3    5  10/26/2017   1:24:55        0.0    1.0  \n",
       "4    5  10/26/2017   0:56:21        0.0    1.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isMad(hour):\n",
    "    format = '%H:%M:%S'\n",
    "    hour = datetime.strptime(hour, format)\n",
    "    initLimit = datetime.strptime(\"23:00:00\", format)\n",
    "    endLimit = datetime.strptime(\"05:00:00\", format)\n",
    "    \n",
    "    if (hour >= initLimit) or (hour < endLimit):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "datasetInFrame['isMad'] = pd.Series(np.zeros(len(datasetInFrame)), index=datasetInFrame.index)\n",
    "\n",
    "for index, row in datasetInFrame.iterrows():\n",
    "    datasetInFrame.loc[index,\"isMad\"] = isMad(row[\"time\"])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramas do Dataset\n",
    "Como uma forma de analisar a influência de tais *features* os histogramas abaixo foram gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência entre Final de Semana (1.0) e Dia de Semana (0.0)\n",
    "Em termos de números, o dataset possui muito mais tweets classificados em dias semana do que dia de final de semana, entretanto quando dados são convertidos em percentual, apresentam uma sensível diferença. Os tweets em finais de semana tendem a ser mais felizes (sensivelmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000000B38165FDA0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000000B381851518>], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Valência em Fim de Semana\n",
    "datasetInFrame.hist(column='val',by='isWeekend', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência na Madrugada (1.0) e Não (0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000000B381790A20>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000000B381E7BCF8>], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Valência na Madrugada\n",
    "datasetInFrame.hist(column='val',by='isMad', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Cidade\n",
    "#datasetInFrame.hist(column='val',by='cit', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Intensidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Intensidade\n",
    "#datasetInFrame.hist(column='val',by='int', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Dia\n",
    "#datasetInFrame.hist(column='val',by='date', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento  das amostras\n",
    "Outra etapa importante do pré-processamento é o balanceamento dos dados, ou seja, realizar uma distribuição mais uniforme de amostras para as classes de valência. Para tornar mais balanceado, foram selecionadas até 100 amostras de cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>joana fez eu me arrumar atoa affff cr</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>0:45:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão sem paciência sem.or</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/20/2017</td>\n",
       "      <td>23:25:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>até gosto de calor mas tá de mais mds scrr</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>22:36:52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô com uma dorzinha d cabeça q ta me dx enjoad...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/18/2017</td>\n",
       "      <td>22:33:22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  cit  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3    1   \n",
       "1  line_fdl              joana fez eu me arrumar atoa affff cr    3    5    1   \n",
       "2  line_fdl                        tô tão sem paciência sem.or    3    5    1   \n",
       "3  line_fdl         até gosto de calor mas tá de mais mds scrr    3    4    1   \n",
       "4  line_fdl  tô com uma dorzinha d cabeça q ta me dx enjoad...    3    4    1   \n",
       "\n",
       "   dia        date      time  isWeekend  isMad  \n",
       "0    6  10/27/2017   0:17:49        0.0    1.0  \n",
       "1    4  10/25/2017   0:45:39        0.0    1.0  \n",
       "2    6  10/20/2017  23:25:23        1.0    1.0  \n",
       "3    5  10/19/2017  22:36:52        0.0    0.0  \n",
       "4    4  10/18/2017  22:33:22        0.0    0.0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedDataset = pd.DataFrame()\n",
    "limitSamples = 100\n",
    "for i in datasetInFrame['val'].unique():\n",
    "    balancedDataset = balancedDataset.append(datasetInFrame[datasetInFrame['val'] == i].iloc[:limitSamples],ignore_index=True)\n",
    "    \n",
    "balancedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb381e7bcf8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x='val', data=balancedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb381e7bcf8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seleciona aleatóriamente\n",
    "dataset = balancedDataset.sample(len(balancedDataset), replace=True)\n",
    "sns.countplot(x='val', data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>sdds sair p sambar, tô precisando..</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/22/2017</td>\n",
       "      <td>14:49:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>FabricioGemada</td>\n",
       "      <td>vai rola o churrasco dos crias</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10/24/2017</td>\n",
       "      <td>22:50:29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>breendoo18</td>\n",
       "      <td>crlh meu telefone voltoooooou porraaaaaaaaa</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10/21/2017</td>\n",
       "      <td>14:17:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>anneray_hd</td>\n",
       "      <td>fico cada dia mais decepcionada e assustada co...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>21:13:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>brunaarvi</td>\n",
       "      <td>ao invés de ficar me frustrando e comentando s...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>0:40:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                txt  val  \\\n",
       "304        line_fdl                sdds sair p sambar, tô precisando..    2   \n",
       "134  FabricioGemada                     vai rola o churrasco dos crias    5   \n",
       "565      breendoo18        crlh meu telefone voltoooooou porraaaaaaaaa    7   \n",
       "394      anneray_hd  fico cada dia mais decepcionada e assustada co...    2   \n",
       "120       brunaarvi  ao invés de ficar me frustrando e comentando s...    5   \n",
       "\n",
       "     int  cit  dia        date      time  isWeekend  isMad  \n",
       "304    3    1    1  10/22/2017  14:49:21        1.0    0.0  \n",
       "134    3    1    3  10/24/2017  22:50:29        0.0    0.0  \n",
       "565    7    1    7  10/21/2017  14:17:05        1.0    0.0  \n",
       "394    5    1    4  10/25/2017  21:13:55        0.0    0.0  \n",
       "120    4    1    5  10/19/2017   0:40:39        0.0    1.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos tweets\n",
    "Remoção de caracteres especiais e conversão em letras minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>sdds sair p sambar tô precisando</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/22/2017</td>\n",
       "      <td>14:49:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>FabricioGemada</td>\n",
       "      <td>vai rola o churrasco dos crias</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10/24/2017</td>\n",
       "      <td>22:50:29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>breendoo18</td>\n",
       "      <td>crlh meu telefone voltoooooou porraaaaaaaaa</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10/21/2017</td>\n",
       "      <td>14:17:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>anneray_hd</td>\n",
       "      <td>fico cada dia mais decepcionada e assustada co...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>21:13:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>brunaarvi</td>\n",
       "      <td>ao invés de ficar me frustrando e comentando s...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>0:40:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                txt  val  \\\n",
       "304        line_fdl                   sdds sair p sambar tô precisando    2   \n",
       "134  FabricioGemada                     vai rola o churrasco dos crias    5   \n",
       "565      breendoo18        crlh meu telefone voltoooooou porraaaaaaaaa    7   \n",
       "394      anneray_hd  fico cada dia mais decepcionada e assustada co...    2   \n",
       "120       brunaarvi  ao invés de ficar me frustrando e comentando s...    5   \n",
       "\n",
       "     int  cit  dia        date      time  isWeekend  isMad  \n",
       "304    3    1    1  10/22/2017  14:49:21        1.0    0.0  \n",
       "134    3    1    3  10/24/2017  22:50:29        0.0    0.0  \n",
       "565    7    1    7  10/21/2017  14:17:05        1.0    0.0  \n",
       "394    5    1    4  10/25/2017  21:13:55        0.0    0.0  \n",
       "120    4    1    5  10/19/2017   0:40:39        0.0    1.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [str(sentence).lower().replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace('\"',\"\").replace(\"?\",\"\") for sentence in dataset[\"txt\"]]\n",
    "dataset[\"txt\"] = sentences\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>[sdds, sair, p, sambar, tô, precisando]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/22/2017</td>\n",
       "      <td>14:49:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>FabricioGemada</td>\n",
       "      <td>[vai, rola, o, churrasco, dos, crias]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10/24/2017</td>\n",
       "      <td>22:50:29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>breendoo18</td>\n",
       "      <td>[crlh, meu, telefone, voltoooooou, porraaaaaaaaa]</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10/21/2017</td>\n",
       "      <td>14:17:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>anneray_hd</td>\n",
       "      <td>[fico, cada, dia, mais, decepcionada, e, assus...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>21:13:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>brunaarvi</td>\n",
       "      <td>[ao, invés, de, ficar, me, frustrando, e, come...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>0:40:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                txt  val  \\\n",
       "304        line_fdl            [sdds, sair, p, sambar, tô, precisando]    2   \n",
       "134  FabricioGemada              [vai, rola, o, churrasco, dos, crias]    5   \n",
       "565      breendoo18  [crlh, meu, telefone, voltoooooou, porraaaaaaaaa]    7   \n",
       "394      anneray_hd  [fico, cada, dia, mais, decepcionada, e, assus...    2   \n",
       "120       brunaarvi  [ao, invés, de, ficar, me, frustrando, e, come...    5   \n",
       "\n",
       "     int  cit  dia        date      time  isWeekend  isMad  \n",
       "304    3    1    1  10/22/2017  14:49:21        1.0    0.0  \n",
       "134    3    1    3  10/24/2017  22:50:29        0.0    0.0  \n",
       "565    7    1    7  10/21/2017  14:17:05        1.0    0.0  \n",
       "394    5    1    4  10/25/2017  21:13:55        0.0    0.0  \n",
       "120    4    1    5  10/19/2017   0:40:39        0.0    1.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencesWithTokens = [nltk.word_tokenize(sentence.lower()) for sentence in dataset[\"txt\"]]\n",
    "dataset[\"txt\"] = sentencesWithTokens\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção das StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>sdds sair p sambar tô precisando</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/22/2017</td>\n",
       "      <td>14:49:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>FabricioGemada</td>\n",
       "      <td>vai rola churrasco crias</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10/24/2017</td>\n",
       "      <td>22:50:29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>breendoo18</td>\n",
       "      <td>crlh telefone voltoooooou porraaaaaaaaa</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10/21/2017</td>\n",
       "      <td>14:17:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>anneray_hd</td>\n",
       "      <td>fico cada dia decepcionada assustada rumo mund...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>21:13:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>brunaarvi</td>\n",
       "      <td>invés ficar frustrando comentando sobre algo q...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>0:40:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                txt  val  \\\n",
       "304        line_fdl                   sdds sair p sambar tô precisando    2   \n",
       "134  FabricioGemada                           vai rola churrasco crias    5   \n",
       "565      breendoo18            crlh telefone voltoooooou porraaaaaaaaa    7   \n",
       "394      anneray_hd  fico cada dia decepcionada assustada rumo mund...    2   \n",
       "120       brunaarvi  invés ficar frustrando comentando sobre algo q...    5   \n",
       "\n",
       "     int  cit  dia        date      time  isWeekend  isMad  \n",
       "304    3    1    1  10/22/2017  14:49:21        1.0    0.0  \n",
       "134    3    1    3  10/24/2017  22:50:29        0.0    0.0  \n",
       "565    7    1    7  10/21/2017  14:17:05        1.0    0.0  \n",
       "394    5    1    4  10/25/2017  21:13:55        0.0    0.0  \n",
       "120    4    1    5  10/19/2017   0:40:39        0.0    1.0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = set(stopwords.words(\"portuguese\"))\n",
    "#words = ' '.join([w for w in words if not w in stops])\n",
    "\n",
    "sentencesWithoutStopWords = []\n",
    "for i,row in dataset.iterrows():\n",
    "    sentence =  ' '.join([w for w in row[\"txt\"] if not w in stops])\n",
    "    sentencesWithoutStopWords.append(sentence.strip())\n",
    "dataset[\"txt\"]=sentencesWithoutStopWords\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Features* de Texto\n",
    "\n",
    "Uma comum abordagem de classificação de texto é a utilização de *Bag of Words*, que fará a contagem da ocorrência de uma palavra nos dados de treino, e em seguida, o *Term Frequency-Inverse Document Frequency* que divide o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento. Como estratégia, escolhemos utilizar Bigrams(ngram), que considerará a ocorrência cada palavra e a anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Bag Of Words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(token_pattern=r\"(?u)\\b[a-zA-Z]\\w+\\b\",lowercase=True, ngram_range=(1, 2))\n",
    "datasetTrain = count_vect.fit_transform(dataset[\"txt\"])\n",
    "\n",
    "###Visualização\n",
    "#from pprint import pprint\n",
    "#pprint(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de features:  (569, 2652)\n"
     ]
    }
   ],
   "source": [
    "#datasetTrain.todense()\n",
    "print(\"Total de features: \", datasetTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Term Frequency-Inverse Document Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm=\"l2\",use_idf=True)\n",
    "X_tfidf = tfidf_transformer.fit_transform(datasetTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação do dataset\n",
    "A abordagem definida foi de 70% para treino e 30% para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = datasetTrain[:int(datasetTrain.shape[0] * 0.7)]\n",
    "trainTarget = dataset['val'][:int(len(dataset['val']) * 0.7)]\n",
    "\n",
    "test = datasetTrain[:-int(datasetTrain.shape[0] * 0.3)]\n",
    "testTarget = dataset['val'][:-int(len(dataset['val']) * 0.3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem com SVM\n",
    "A abordagem com o SVM foi escolhida, pois ele apresenta bons resultados e por experiências anteriores com o algoritmo.\n",
    "\n",
    "A implementação utilizada disponível em [scikit-learn](http://scikit-learn.org/stable/index.html) uma biblioteca em Python amplamente utilizada para tarefas de *Machine Learning*.\n",
    "\n",
    "Parâmetros:\n",
    "* Kernel = 'rbf', recomendado por algumas referências [1] como uma boa escolha de primeira abordagem. O \"poly\" teve um desempenho muito ruim e o \"linear\" apresentou um score alto.\n",
    "* Gamma = 0, apresentou bons resultados quando \"rbf\"\n",
    "* C = 30, apresentou bons resultados quando \"rbf\", quando 100 aproximou de 98%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm = SVC(kernel=\"linear\") # ~ 99 \n",
    "#svm = SVC(kernel=\"poly\") # ~ 19 \n",
    "#svm = SVC(kernel='rbf',gamma=0.001, C=100.) # ~ 98\n",
    "svm = SVC(kernel='rbf',gamma=0.001, C=30.) # ~87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features : 2652\n",
      "Score:  0.84962406015\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of features : %d\" % train.shape[1])\n",
    "svm.fit(train, trainTarget)\n",
    "predict = svm.predict(test)\n",
    "print(\"Score: \",svm.score(test, testTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificado   1    2   3   4   5   6  7  All\n",
      "Real                                         \n",
      "1             24   16   0   0   0   0  0   40\n",
      "2              0   80   1   0   0   0  0   81\n",
      "3              0    3  72   0   0   0  0   75\n",
      "4              0   13   1  34   2   0  0   50\n",
      "5              0    4   0   0  71   0  0   75\n",
      "6              0    8   1   0   2  49  0   60\n",
      "7              0    6   0   0   1   2  9   18\n",
      "All           24  130  75  34  76  51  9  399\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(testTarget, predict, rownames=['Real'], colnames=['Classificado'], margins=True))\n",
    "#print(confusion_matrix(testTarget, predict, labels = [1, 2, 3,4,5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "      <th>cit</th>\n",
       "      <th>dia</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>isWeekend</th>\n",
       "      <th>isMad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão pensativa ultimamente, nn tô me sentind...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/27/2017</td>\n",
       "      <td>0:17:49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>joana fez eu me arrumar atoa affff cr</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/25/2017</td>\n",
       "      <td>0:45:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô tão sem paciência sem.or</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10/20/2017</td>\n",
       "      <td>23:25:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>até gosto de calor mas tá de mais mds scrr</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10/19/2017</td>\n",
       "      <td>22:36:52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line_fdl</td>\n",
       "      <td>tô com uma dorzinha d cabeça q ta me dx enjoad...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10/18/2017</td>\n",
       "      <td>22:33:22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                txt  val  int  cit  \\\n",
       "0  line_fdl  tô tão pensativa ultimamente, nn tô me sentind...    3    3    1   \n",
       "1  line_fdl              joana fez eu me arrumar atoa affff cr    3    5    1   \n",
       "2  line_fdl                        tô tão sem paciência sem.or    3    5    1   \n",
       "3  line_fdl         até gosto de calor mas tá de mais mds scrr    3    4    1   \n",
       "4  line_fdl  tô com uma dorzinha d cabeça q ta me dx enjoad...    3    4    1   \n",
       "\n",
       "   dia        date      time  isWeekend  isMad  \n",
       "0    6  10/27/2017   0:17:49        0.0    1.0  \n",
       "1    4  10/25/2017   0:45:39        0.0    1.0  \n",
       "2    6  10/20/2017  23:25:23        1.0    1.0  \n",
       "3    5  10/19/2017  22:36:52        0.0    0.0  \n",
       "4    4  10/18/2017  22:33:22        0.0    0.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação\n",
    "\n",
    "Para validar o classificador foi utilizado um dataset sem labels, o dataset é composto das seguintes colunas:\n",
    "\n",
    "1. id - Usuário no Twitter\n",
    "2. txt - O conteúdo do tweet\n",
    "3. cit - Cidade e País do usuário\n",
    "4. data - Data e hora do tweet\n",
    "5. dia - Dia da semana\n",
    "\n",
    "Total: 4202 amostras x 5 colunas.\n",
    "\n",
    "O dataset passou pelo mesmo pré-processamento de emoji que o dataset utilizado para treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetValida = pd.read_csv(\"demojized_input-emoji.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)\n",
    "\n",
    "### Colocando espaços entre os emojis\n",
    "sentencesEmojis = [str(sentence).lower().replace(\":\",\" \") for sentence in datasetValida[\"txt\"]]\n",
    "datasetValida[\"txt\"] = sentencesEmojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = datasetValida[\"txt\"]\n",
    "validaCounts = count_vect.transform(tweets)\n",
    "validaTfidf = tfidf_transformer.transform(validaCounts)\n",
    "\n",
    "validaPredict = svm.predict(validaTfidf)\n",
    "\n",
    "for doc, category in zip(tweets, validaPredict):\n",
    "    #print(doc, \" \", category)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valência e Intensidade\n",
    "A valência foi predita pelo modelo e a intensidade adotada como padrão foi a 4 (neutra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>cit</th>\n",
       "      <th>data</th>\n",
       "      <th>cit.1</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>apenas um filme lésbico pode me alegrar neste ...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>27/10/2017 00:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>fodase só queria alguém que me amasse e aceita...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>27/10/2017 00:08</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>já deu pra mim, já tentei de todas as maneiras...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>27/10/2017 00:01</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>se liga vou fazer um drama</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>27/10/2017 00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>me sentindo levemente suicida</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>26/10/2017 23:57</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                txt  \\\n",
       "0  onnabia  apenas um filme lésbico pode me alegrar neste ...   \n",
       "1  onnabia  fodase só queria alguém que me amasse e aceita...   \n",
       "2  onnabia  já deu pra mim, já tentei de todas as maneiras...   \n",
       "3  onnabia                         se liga vou fazer um drama   \n",
       "4  onnabia                      me sentindo levemente suicida   \n",
       "\n",
       "                      cit              data  cit.1  val  int  \n",
       "0  Rio de Janeiro, Brasil  27/10/2017 00:14      6    2    4  \n",
       "1  Rio de Janeiro, Brasil  27/10/2017 00:08      6    2    4  \n",
       "2  Rio de Janeiro, Brasil  27/10/2017 00:01      6    2    4  \n",
       "3  Rio de Janeiro, Brasil  27/10/2017 00:00      6    2    4  \n",
       "4  Rio de Janeiro, Brasil  26/10/2017 23:57      5    2    4  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValida[\"val\"] = pd.Series(validaPredict, index=datasetValida.index)\n",
    "\n",
    "#Todas as intensidades como 4\n",
    "intensity = [4]*len(datasetValida)\n",
    "datasetValida[\"int\"] = pd.Series(intensity, index=datasetValida.index)\n",
    "\n",
    "datasetValida.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de dados para saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>cit</th>\n",
       "      <th>data</th>\n",
       "      <th>cit.1</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>apenas um filme lésbico pode me alegrar neste ...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>fodase só queria alguém que me amasse e aceita...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:08</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>já deu pra mim, já tentei de todas as maneiras...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:01</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>se liga vou fazer um drama</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>me sentindo levemente suicida</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 23:57</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>chapada de zolpidem graças a deus</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 23:29</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>to lendo aqui freud a interpretação dos sonhos...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>“sonhar com borboletas  preste mais atenção no...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:33</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>eu não entendo quem criou que eu sonhar com de...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>vcs acreditam que sonhos têm significados?</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:31</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>o tempo todo eu sonho que to sendo atacada por...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:29</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>pornô indie</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>eu tava tão bem sem pensar em sexo pqp vtnc</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>boa noite quer transar ?</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 22:08</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>devolve minha libido não quero mais</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 21:48</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>ah meu deus q tesao</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 21:48</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>só quero tatuar o pescoço, perder 25kg e minha...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 21:42</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>a vida é injusta né</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 21:35</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>no momento apoiando a cena</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 21:08</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>pc siqueira x mombaque</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 20:49</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>que vontade horrível de chorar misericórdia</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 19:59</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>caraca no momento sendo a mulher mais feia do ...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 19:12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>meu deus to há muito tempo lendo o perfil de u...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 17:49</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>caraca deixa eu ver se eu entendi o cara tatuo...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 14:32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>e agora, o que eu faço?</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 14:27</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>estamos namorando a partir deste tweet</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 14:25</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>vamos namorar</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 14:25</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>fala seguidores</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 12:05</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>quero casar com o @temerpoeta</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 00:21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>eu não entendo muito bem o conceito de precisa...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/25/17 22:51</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>ja acordo refletindo q tenho 2 irmãos que caga...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/21/17 12:39</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>serio bgl q me deixa mais na bad sincera e div...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/21/17 12:37</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>la vou eu ficar com varias conta atrasadas for...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/21/17 12:34</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>mano serio nao acredito q me planejei todinha ...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/21/17 12:34</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>tem dias q eu me acho pior ser humano existente</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 18:55</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>to de bode comigo mesma</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 18:54</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>n gosto da transferência do metrô p cptm de pi...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 18:53</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>aquele jet pra osasco bem suav</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 18:53</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>tava vendo um insta de desenhos de casais tran...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 15:47</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>minha auto estima esta uma boista hj</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 14:19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>toda vez q sento no banco preferencial penso q...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 14:19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>fiz maior corre da vida p levar um bolo em ita...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/20/17 14:07</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>feliz e cansada</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/19/17 22:26</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>ond vejo nba online ?</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/19/17 01:15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>tempos q qro fazer um bullet journal aí vi o v...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/19/17 01:07</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>nunca ganhei flores mas nossa é ilusão acha q ...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 21:36</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>deve ser triste te um rabao e n sabe rebola</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 21:33</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>alguém aí joga guns of boom?</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 21:30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>comida asiática pisa muito</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 15:21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>ninguém merece ouvir legião urbana no trabalho...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 14:02</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>renovada depois de sair da psico</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 12:20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>acordei tipo como hoje mjjjjjkkk</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 10:59</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>tem que ser muito egocêntrica para achar que u...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 10:58</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>n consigo mais digerir grupos feministas pq n ...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 10:52</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>bom diaaa</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/18/17 10:44</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>chega em casa e come arroz com linguisa</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/17/17 23:02</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>no dia q n precisa ir de social n posso ir con...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/17/17 22:57</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>eu ainda vou lança looks bem rihanninha execut...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/17/17 22:56</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>alguém boto fogo na marechal ta cherando macon...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/17/17 22:47</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>dirtylixo</td>\n",
       "      <td>xopin trem pisa muito obrigada ai mano q salvo...</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>10/17/17 22:25</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4202 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                                txt  \\\n",
       "0       onnabia  apenas um filme lésbico pode me alegrar neste ...   \n",
       "1       onnabia  fodase só queria alguém que me amasse e aceita...   \n",
       "2       onnabia  já deu pra mim, já tentei de todas as maneiras...   \n",
       "3       onnabia                         se liga vou fazer um drama   \n",
       "4       onnabia                      me sentindo levemente suicida   \n",
       "5       onnabia                  chapada de zolpidem graças a deus   \n",
       "6       onnabia  to lendo aqui freud a interpretação dos sonhos...   \n",
       "7       onnabia  “sonhar com borboletas  preste mais atenção no...   \n",
       "8       onnabia  eu não entendo quem criou que eu sonhar com de...   \n",
       "9       onnabia         vcs acreditam que sonhos têm significados?   \n",
       "10      onnabia  o tempo todo eu sonho que to sendo atacada por...   \n",
       "11      onnabia                                        pornô indie   \n",
       "12      onnabia        eu tava tão bem sem pensar em sexo pqp vtnc   \n",
       "13      onnabia                           boa noite quer transar ?   \n",
       "14      onnabia                devolve minha libido não quero mais   \n",
       "15      onnabia                                ah meu deus q tesao   \n",
       "16      onnabia  só quero tatuar o pescoço, perder 25kg e minha...   \n",
       "17      onnabia                                a vida é injusta né   \n",
       "18      onnabia                         no momento apoiando a cena   \n",
       "19      onnabia                             pc siqueira x mombaque   \n",
       "20      onnabia        que vontade horrível de chorar misericórdia   \n",
       "21      onnabia  caraca no momento sendo a mulher mais feia do ...   \n",
       "22      onnabia  meu deus to há muito tempo lendo o perfil de u...   \n",
       "23      onnabia  caraca deixa eu ver se eu entendi o cara tatuo...   \n",
       "24      onnabia                            e agora, o que eu faço?   \n",
       "25      onnabia             estamos namorando a partir deste tweet   \n",
       "26      onnabia                                      vamos namorar   \n",
       "27      onnabia                                    fala seguidores   \n",
       "28      onnabia                      quero casar com o @temerpoeta   \n",
       "29      onnabia  eu não entendo muito bem o conceito de precisa...   \n",
       "...         ...                                                ...   \n",
       "4172  dirtylixo  ja acordo refletindo q tenho 2 irmãos que caga...   \n",
       "4173  dirtylixo  serio bgl q me deixa mais na bad sincera e div...   \n",
       "4174  dirtylixo  la vou eu ficar com varias conta atrasadas for...   \n",
       "4175  dirtylixo  mano serio nao acredito q me planejei todinha ...   \n",
       "4176  dirtylixo    tem dias q eu me acho pior ser humano existente   \n",
       "4177  dirtylixo                            to de bode comigo mesma   \n",
       "4178  dirtylixo  n gosto da transferência do metrô p cptm de pi...   \n",
       "4179  dirtylixo                     aquele jet pra osasco bem suav   \n",
       "4180  dirtylixo  tava vendo um insta de desenhos de casais tran...   \n",
       "4181  dirtylixo               minha auto estima esta uma boista hj   \n",
       "4182  dirtylixo  toda vez q sento no banco preferencial penso q...   \n",
       "4183  dirtylixo  fiz maior corre da vida p levar um bolo em ita...   \n",
       "4184  dirtylixo                                    feliz e cansada   \n",
       "4185  dirtylixo                              ond vejo nba online ?   \n",
       "4186  dirtylixo  tempos q qro fazer um bullet journal aí vi o v...   \n",
       "4187  dirtylixo  nunca ganhei flores mas nossa é ilusão acha q ...   \n",
       "4188  dirtylixo        deve ser triste te um rabao e n sabe rebola   \n",
       "4189  dirtylixo                       alguém aí joga guns of boom?   \n",
       "4190  dirtylixo                         comida asiática pisa muito   \n",
       "4191  dirtylixo  ninguém merece ouvir legião urbana no trabalho...   \n",
       "4192  dirtylixo                   renovada depois de sair da psico   \n",
       "4193  dirtylixo                   acordei tipo como hoje mjjjjjkkk   \n",
       "4194  dirtylixo  tem que ser muito egocêntrica para achar que u...   \n",
       "4195  dirtylixo  n consigo mais digerir grupos feministas pq n ...   \n",
       "4196  dirtylixo                                          bom diaaa   \n",
       "4197  dirtylixo            chega em casa e come arroz com linguisa   \n",
       "4198  dirtylixo  no dia q n precisa ir de social n posso ir con...   \n",
       "4199  dirtylixo  eu ainda vou lança looks bem rihanninha execut...   \n",
       "4200  dirtylixo  alguém boto fogo na marechal ta cherando macon...   \n",
       "4201  dirtylixo  xopin trem pisa muito obrigada ai mano q salvo...   \n",
       "\n",
       "                         cit            data  cit.1  val  int  \n",
       "0     Rio de Janeiro, Brasil  10/27/17 00:14      6    2    4  \n",
       "1     Rio de Janeiro, Brasil  10/27/17 00:08      6    2    4  \n",
       "2     Rio de Janeiro, Brasil  10/27/17 00:01      6    2    4  \n",
       "3     Rio de Janeiro, Brasil  10/27/17 00:00      6    2    4  \n",
       "4     Rio de Janeiro, Brasil  10/26/17 23:57      5    2    4  \n",
       "5     Rio de Janeiro, Brasil  10/26/17 23:29      5    2    4  \n",
       "6     Rio de Janeiro, Brasil  10/26/17 22:38      5    2    4  \n",
       "7     Rio de Janeiro, Brasil  10/26/17 22:33      5    2    4  \n",
       "8     Rio de Janeiro, Brasil  10/26/17 22:32      5    2    4  \n",
       "9     Rio de Janeiro, Brasil  10/26/17 22:31      5    2    4  \n",
       "10    Rio de Janeiro, Brasil  10/26/17 22:29      5    2    4  \n",
       "11    Rio de Janeiro, Brasil  10/26/17 22:14      5    2    4  \n",
       "12    Rio de Janeiro, Brasil  10/26/17 22:12      5    2    4  \n",
       "13    Rio de Janeiro, Brasil  10/26/17 22:08      5    2    4  \n",
       "14    Rio de Janeiro, Brasil  10/26/17 21:48      5    2    4  \n",
       "15    Rio de Janeiro, Brasil  10/26/17 21:48      5    2    4  \n",
       "16    Rio de Janeiro, Brasil  10/26/17 21:42      5    2    4  \n",
       "17    Rio de Janeiro, Brasil  10/26/17 21:35      5    2    4  \n",
       "18    Rio de Janeiro, Brasil  10/26/17 21:08      5    2    4  \n",
       "19    Rio de Janeiro, Brasil  10/26/17 20:49      5    2    4  \n",
       "20    Rio de Janeiro, Brasil  10/26/17 19:59      5    2    4  \n",
       "21    Rio de Janeiro, Brasil  10/26/17 19:12      5    2    4  \n",
       "22    Rio de Janeiro, Brasil  10/26/17 17:49      5    2    4  \n",
       "23    Rio de Janeiro, Brasil  10/26/17 14:32      5    2    4  \n",
       "24    Rio de Janeiro, Brasil  10/26/17 14:27      5    2    4  \n",
       "25    Rio de Janeiro, Brasil  10/26/17 14:25      5    2    4  \n",
       "26    Rio de Janeiro, Brasil  10/26/17 14:25      5    2    4  \n",
       "27    Rio de Janeiro, Brasil  10/26/17 12:05      5    2    4  \n",
       "28    Rio de Janeiro, Brasil  10/26/17 00:21      5    2    4  \n",
       "29    Rio de Janeiro, Brasil  10/25/17 22:51      4    2    4  \n",
       "...                      ...             ...    ...  ...  ...  \n",
       "4172       São Paulo, Brasil  10/21/17 12:39      7    2    4  \n",
       "4173       São Paulo, Brasil  10/21/17 12:37      7    2    4  \n",
       "4174       São Paulo, Brasil  10/21/17 12:34      7    2    4  \n",
       "4175       São Paulo, Brasil  10/21/17 12:34      7    2    4  \n",
       "4176       São Paulo, Brasil  10/20/17 18:55      6    2    4  \n",
       "4177       São Paulo, Brasil  10/20/17 18:54      6    2    4  \n",
       "4178       São Paulo, Brasil  10/20/17 18:53      6    2    4  \n",
       "4179       São Paulo, Brasil  10/20/17 18:53      6    2    4  \n",
       "4180       São Paulo, Brasil  10/20/17 15:47      6    2    4  \n",
       "4181       São Paulo, Brasil  10/20/17 14:19      6    2    4  \n",
       "4182       São Paulo, Brasil  10/20/17 14:19      6    2    4  \n",
       "4183       São Paulo, Brasil  10/20/17 14:07      6    2    4  \n",
       "4184       São Paulo, Brasil  10/19/17 22:26      5    2    4  \n",
       "4185       São Paulo, Brasil  10/19/17 01:15      5    2    4  \n",
       "4186       São Paulo, Brasil  10/19/17 01:07      5    2    4  \n",
       "4187       São Paulo, Brasil  10/18/17 21:36      4    2    4  \n",
       "4188       São Paulo, Brasil  10/18/17 21:33      4    2    4  \n",
       "4189       São Paulo, Brasil  10/18/17 21:30      4    2    4  \n",
       "4190       São Paulo, Brasil  10/18/17 15:21      4    2    4  \n",
       "4191       São Paulo, Brasil  10/18/17 14:02      4    2    4  \n",
       "4192       São Paulo, Brasil  10/18/17 12:20      4    2    4  \n",
       "4193       São Paulo, Brasil  10/18/17 10:59      4    2    4  \n",
       "4194       São Paulo, Brasil  10/18/17 10:58      4    2    4  \n",
       "4195       São Paulo, Brasil  10/18/17 10:52      4    2    4  \n",
       "4196       São Paulo, Brasil  10/18/17 10:44      4    2    4  \n",
       "4197       São Paulo, Brasil  10/17/17 23:02      3    2    4  \n",
       "4198       São Paulo, Brasil  10/17/17 22:57      3    2    4  \n",
       "4199       São Paulo, Brasil  10/17/17 22:56      3    2    4  \n",
       "4200       São Paulo, Brasil  10/17/17 22:47      3    2    4  \n",
       "4201       São Paulo, Brasil  10/17/17 22:25      3    2    4  \n",
       "\n",
       "[4202 rows x 7 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValida[\"txt\"] = sentencesEmojis\n",
    "datasetValida.head()\n",
    "\n",
    "formato = '%d/%m/%Y %H:%M'\n",
    "dateTimeConverted = [datetime.strptime(dt, formato).strftime('%m/%d/%y %H:%M') for dt in datasetValida[\"data\"]]\n",
    "datasetValida[\"data\"] = dateTimeConverted\n",
    "sentencesValida = [str(sent) for sent in datasetValida[\"txt\"]]\n",
    "datasetValida[\"txt\"] = sentencesValida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Histograma das classificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb381e7bcf8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x='val', data=datasetValida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>txt</th>\n",
       "      <th>cit</th>\n",
       "      <th>data</th>\n",
       "      <th>cit.1</th>\n",
       "      <th>val</th>\n",
       "      <th>int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>apenas um filme lésbico pode me alegrar neste ...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>fodase só queria alguém que me amasse e aceita...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:08</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>já deu pra mim, já tentei de todas as maneiras...</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:01</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>se liga vou fazer um drama</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/27/17 00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onnabia</td>\n",
       "      <td>me sentindo levemente suicida</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>10/26/17 23:57</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                                txt  \\\n",
       "0  onnabia  apenas um filme lésbico pode me alegrar neste ...   \n",
       "1  onnabia  fodase só queria alguém que me amasse e aceita...   \n",
       "2  onnabia  já deu pra mim, já tentei de todas as maneiras...   \n",
       "3  onnabia                         se liga vou fazer um drama   \n",
       "4  onnabia                      me sentindo levemente suicida   \n",
       "\n",
       "                      cit            data  cit.1  val  int  \n",
       "0  Rio de Janeiro, Brasil  10/27/17 00:14      6    2    4  \n",
       "1  Rio de Janeiro, Brasil  10/27/17 00:08      6    2    4  \n",
       "2  Rio de Janeiro, Brasil  10/27/17 00:01      6    2    4  \n",
       "3  Rio de Janeiro, Brasil  10/27/17 00:00      6    2    4  \n",
       "4  Rio de Janeiro, Brasil  10/26/17 23:57      5    2    4  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetValida.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo gerado: output-classificador-com-dmoji-87.csv \n"
     ]
    }
   ],
   "source": [
    "datasetValida.to_csv('output-classificador-com-demoji-rbf-85.csv', \",\", index=False)\n",
    "print(\"Arquivo gerado: output-classificador-com-dmoji-85.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "Foi possível obter uma classificação inicial válida e com bons resultados (em muitos casos \"overfitting\") a partir do SVM. Avaliando os diferentes parâmetros selecionados, obtivemos os seguintes resultados:\n",
    "\n",
    "* Kernel Linear - 99% e se apresentando um classificador \"otimista\"\n",
    "* Kernel Polynomial - 19% \n",
    "* Kernel Radial Basis Function (gamma=0.001, C=100) - 98% e se apresentando um classificador \"pessimista\"\n",
    "* Kernel Radial Basis Function (gamma=0.001, c=30) - 87% e classificou apenas as classes 3,4 e 5 (também \"pessimista\")\n",
    "\n",
    "Na matriz de confusão do Kernel Radial Basis Function (gamma=0.001, c=30), o modelo classificou muito bem as classes, errando mais na classe de valência 3, indicando uma forte tendência em classificar os tweets como negativos, o que chamamos aqui de “pessimista”. Outro interessante comportamento do classificador com tais parâmetros, é que classificou apenas as classes 3,4 e 5, o que podemos trocar por Negativo, Neutro e Positivo.\n",
    "\n",
    "Apesar das classificações serem dadas como \"pessimistas\" no caso do RBF, as classificações se mostraram, de certa forma, coerentes com o conteúdo dos tweets.\n",
    "\n",
    "Entre as dificuldades e fraquezas, podem ser citadas o processo de extração de *features*, nesse caso, para se aproximar do objetivo da ferramenta proposta no projeto, devemos estudar e adicionar outras *features* a serem consideradas no classificador, tais como final de semana, madrugada, entre outras que julgarmos relacionadas. \n",
    "No decorrer da implementação, encontramos algumas dificuldades em considerar os emojis, mas ao final da implementação, eles foram adicionados ao BoW e ao classificador.\n",
    "\n",
    "Uma questão importante a ser considerada, é de que os tweets possuem muitas gírias, siglas e erros ortográficos. Dessa forma, acreditamos que, como próximos passos, seja uma avaliação mais profunda no impacto disso no classificador e a resolução de tal problema.\n",
    "Outro ponto importante, como trabalho futuro, é a classificação de intensidade. Na implementação apresentada, apenas a classificação da Valência foi considerada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[1] NLM. Faceli K, Lorena AC, Gama J, Carvalho ACP de LF de. **Inteligência artificial: uma abordagem de aprendizado de máquina**. Rio de Janeiro: LTC, 2011.\n",
    "\n",
    "[2] [Aprendizado de Máquina com Python](https://iascblog.wordpress.com/2017/03/17/aprendizado-de-maquina-supervisionado-com-python/)\n",
    "\n",
    "[3] [Código de Exemplo](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n",
    "\n",
    "[4] [Cross Validation](http://juliocesarbatista.com/post/Cross-validation-testando-o-desempenho-de-um-classificador/)\n",
    "\n",
    "[5] [Scikit-learn - Trabalhando com Textos](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
