{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Valência em Tweets\n",
    "Este Notebook contém os resultados de uma das etapas do Projeto Final da disciplina de \"IA369-Y\" na UNICAMP. São apresentados aqui os resultados da implementação de um classificador de valência (1 a 7) em tweets utilizando uma abordagem de aprendizado de máquina, especificamente, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "O conjunto de dados utilizados nesta implementação são referentes aos tweets importados pela API [Tweepy](http://www.tweepy.org/) e pré-processados. O dataset é composto com as seguintes colunas:\n",
    "\n",
    "1. id - Usuário no Twitter\n",
    "2. txt - O conteúdo do tweet\n",
    "3. val - Valência de 1 a 7\n",
    "4. int - Intensidade de 1 a 7\n",
    "5. cit - Cidade e País do usuário\n",
    "6. data - Data e hora do tweet\n",
    "7. dia - Dia da semana\n",
    "\n",
    "1318 amostras x 7 colunas\n",
    "\n",
    "Para o tratamento dos emojis, foram gerados datasets com diferentes tratamentos:\n",
    "\n",
    "1. **output-pleasure-arousal-labeled**- Os emojis presentes nos tweets são excluídos\n",
    "2. **output-pleasure-arousal-labeled-emoji** - Os emojis são mantidos, porém sem tratamento, foi observado que em uma das etapas eles são desconsiderados\n",
    "3. **demojized_emojios** - Os emojis são tratados pela biblioteca [Demojize](https://github.com/nkmrtty/demojize.py/blob/master/demojize.py), ou seja, são transcritos, por exemplo *:smirking_face:* e *:yellow_heart:*\n",
    "\n",
    "Ao final, o dataset selecionado foi o **demojized_emojios**, pois foi possível incluir os emojis na abordagem adotada pelo classificador. Entendemos que emojis, principalmente nos tweets, carregam informações valiosas sobre a valência e intensidade.\n",
    "\n",
    "## Linguagens e Bibliotecas\n",
    "\n",
    "1. Ambiente: [Anaconda3 4.3.1](https://repo.continuum.io/archive/index.html)\n",
    "2. Linguagem de Programação: [Python 3.3](https://www.python.org/) \n",
    "3. Biblioteca de Dataframe: [Panda 0.19.2](http://pandas.pydata.org/).\n",
    "4. Machine Learning: [Scikit-learn](http://scikit-learn.org/stable/index.html)\n",
    "5. Plotting: [Matplotlib](https://matplotlib.org/)\n",
    "\n",
    "## Abordagem\n",
    "O dataset foi pré-processado, foram extraídas *features* de texto, separados em dados de treinamento e teste, e por fim, implementado um algoritmo de *Supporte Vector Machines* (SVM), conforme detalhes abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "# encoding: iso-8859-1\n",
    "# encoding: win-1252\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "As etapas adotadas para pré-processar o dataset foram:\n",
    "\n",
    "1. Separação de Data e Hora\n",
    "2. Substituição das Cidades por Valores\n",
    "3. Criação de Features [É madrugada e É final de semana]\n",
    "4. Balanceamento das amostras\n",
    "5. Pré-processamento dos tweets\n",
    "6. Tokenizing\n",
    "7. Remoção das StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset com Demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Com Demogize\n",
    "datasetInFrame = pd.read_csv(\"demojized_emojios.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)\n",
    "### Colocando espaços entre os emojis\n",
    "sentencesEmojis = [str(sentence).lower().replace(\":\",\" \") for sentence in datasetInFrame[\"txt\"]]\n",
    "datasetInFrame[\"txt\"] = sentencesEmojis\n",
    "datasetInFrame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset com Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Com emoji\n",
    "#datasetInFrame = pd.read_csv(\"output-pleasure-arousal-labeled-emoji.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sem Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Sem emoji\n",
    "#datasetInFrame = pd.read_csv(\"output-pleasure-arousal-labeled.csv\", sep=\"|\", encoding=\"ISO-8859-1\",quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição das Classes do Dataset\n",
    "- Valência (val): 1 a 7\n",
    "- Intensidade (int): 1 a 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalValClasses = {}\n",
    "\n",
    "for i in range(1,8):\n",
    "    totalValClasses[i] = datasetInFrame[datasetInFrame.val == i][\"txt\"].count()\n",
    "totalValClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='val', data=datasetInFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intensidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalIntClasses = {}\n",
    "\n",
    "for i in range(1,8):\n",
    "    totalIntClasses[i] = datasetInFrame[datasetInFrame.val == i][\"txt\"].count()\n",
    "totalIntClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='int', data=datasetInFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de data e hora\n",
    "Para gerar *features* posteriormente, foi adotada a estratégia separar os dados de data e hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateTime = datasetInFrame[\"data\"].apply(lambda  x: x.split(' '))\n",
    "date = dateTime.apply(lambda x: x[0])\n",
    "time = dateTime.apply(lambda x: x[1])\n",
    "datasetInFrame[\"date\"] = date\n",
    "datasetInFrame[\"time\"] = time\n",
    "del datasetInFrame[\"data\"]\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituição de Valores Para Cidade\n",
    "1. Removendo o País;\n",
    "2. Substituindo: \n",
    "    * Rio de Janeiro = 1 \n",
    "    * São Paulo = 2\n",
    "\n",
    "Essa estratégia foi adotada para incluir a cidade como uma *feature* no classificador. Apesar desse processamento, essa coluna não foi utilizada na implementação do modelo apresentado aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city = datasetInFrame[\"cit\"].apply(lambda x: x.split(','))\n",
    "datasetInFrame[\"cit\"] = city.apply(lambda x: x[0])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetInFrame[\"cit\"] = datasetInFrame[\"cit\"].apply(lambda x: 1 if x == \"Rio de Janeiro\" else 2)\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de  *features*\n",
    "\n",
    "Após o processamento, definimos como *features* interessantes a serem consideradas como Final de Semana e Madrugada. Intuitivamente, os tweets tendem a ser mais felizes nos finais de semana, mas será que tal característica é de fato relevante? Como uma tentativa de responder a essa questão, foram adicionadas duas colunas de:\n",
    "\n",
    "1. isWeekend? [dia 6 após às 19:00, dia 7 e 1 até 19:00 - 1 sim, 0 não\n",
    "2. isMad? [23:00 às 5:00] - 1 sim, 0 não\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isWeekend(day,hour):\n",
    "    format = '%H:%M:%S'\n",
    "    hour = datetime.strptime(hour, format)\n",
    "    limitWeekend = datetime.strptime(\"19:00:00\", format)\n",
    "    if (day == 6) and (hour >= limitWeekend):\n",
    "        return 1\n",
    "    elif (day == 1) and (hour < limitWeekend):\n",
    "        return 1\n",
    "    elif day == 7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "datasetInFrame['isWeekend'] = pd.Series(np.zeros(len(datasetInFrame)), index=datasetInFrame.index)\n",
    "\n",
    "for index, row in datasetInFrame.iterrows():\n",
    "    datasetInFrame.loc[index,\"isWeekend\"] = isWeekend(row[\"dia\"],row[\"time\"])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isMad(hour):\n",
    "    format = '%H:%M:%S'\n",
    "    hour = datetime.strptime(hour, format)\n",
    "    initLimit = datetime.strptime(\"23:00:00\", format)\n",
    "    endLimit = datetime.strptime(\"05:00:00\", format)\n",
    "    \n",
    "    if (hour >= initLimit) or (hour < endLimit):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "datasetInFrame['isMad'] = pd.Series(np.zeros(len(datasetInFrame)), index=datasetInFrame.index)\n",
    "\n",
    "for index, row in datasetInFrame.iterrows():\n",
    "    datasetInFrame.loc[index,\"isMad\"] = isMad(row[\"time\"])\n",
    "datasetInFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramas do Dataset\n",
    "Como uma forma de analisar a influência de tais *features* os histogramas abaixo foram gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência entre Final de Semana (1.0) e Dia de Semana (0.0)\n",
    "Em termos de números, o dataset possui muito mais tweets classificados em dias semana do que dia de final de semana, entretanto quando dados são convertidos em percentual, apresentam uma sensível diferença. Os tweets em finais de semana tendem a ser mais felizes (sensivelmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência em Fim de Semana\n",
    "datasetInFrame.hist(column='val',by='isWeekend', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência na Madrugada (1.0) e Não (0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência na Madrugada\n",
    "datasetInFrame.hist(column='val',by='isMad', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Cidade\n",
    "#datasetInFrame.hist(column='val',by='cit', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Intensidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Intensidade\n",
    "#datasetInFrame.hist(column='val',by='int', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valência por Dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Valência por Dia\n",
    "#datasetInFrame.hist(column='val',by='date', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento  das amostras\n",
    "Outra etapa importante do pré-processamento é o balanceamento dos dados, ou seja, realizar uma distribuição mais uniforme de amostras para as classes de valência. Para tornar mais balanceado, foram selecionadas até 100 amostras de cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "balancedDataset = pd.DataFrame()\n",
    "limitSamples = 100\n",
    "for i in datasetInFrame['val'].unique():\n",
    "    balancedDataset = balancedDataset.append(datasetInFrame[datasetInFrame['val'] == i].iloc[:limitSamples],ignore_index=True)\n",
    "    \n",
    "balancedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='val', data=balancedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Seleciona aleatóriamente\n",
    "dataset = balancedDataset.sample(len(balancedDataset), replace=True)\n",
    "sns.countplot(x='val', data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos tweets\n",
    "Remoção de caracteres especiais e conversão em letras minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [str(sentence).lower().replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace('\"',\"\").replace(\"?\",\"\") for sentence in dataset[\"txt\"]]\n",
    "dataset[\"txt\"] = sentences\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentencesWithTokens = [nltk.word_tokenize(sentence.lower()) for sentence in dataset[\"txt\"]]\n",
    "dataset[\"txt\"] = sentencesWithTokens\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção das StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"portuguese\"))\n",
    "#words = ' '.join([w for w in words if not w in stops])\n",
    "\n",
    "sentencesWithoutStopWords = []\n",
    "for i,row in dataset.iterrows():\n",
    "    sentence =  ' '.join([w for w in row[\"txt\"] if not w in stops])\n",
    "    sentencesWithoutStopWords.append(sentence.strip())\n",
    "dataset[\"txt\"]=sentencesWithoutStopWords\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Features* de Texto\n",
    "\n",
    "Uma comum abordagem de classificação de texto é a utilização de *Bag of Words*, que fará a contagem da ocorrência de uma palavra nos dados de treino, e em seguida, o *Term Frequency-Inverse Document Frequency* que divide o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento. Como estratégia, escolhemos utilizar Bigrams(ngram), que considerará a ocorrência cada palavra e a anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Bag Of Words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(token_pattern=r\"(?u)\\b[a-zA-Z]\\w+\\b\",lowercase=True, ngram_range=(1, 2))\n",
    "datasetTrain = count_vect.fit_transform(dataset[\"txt\"])\n",
    "\n",
    "###Visualização\n",
    "#from pprint import pprint\n",
    "#pprint(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasetTrain.todense()\n",
    "print(\"Total de features: \", datasetTrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Term Frequency-Inverse Document Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm=\"l2\",use_idf=True)\n",
    "X_tfidf = tfidf_transformer.fit_transform(datasetTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação do dataset\n",
    "A abordagem definida foi de 70% para treino e 30% para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = datasetTrain[:int(datasetTrain.shape[0] * 0.7)]\n",
    "trainTarget = dataset['val'][:int(len(dataset['val']) * 0.7)]\n",
    "\n",
    "test = datasetTrain[:-int(datasetTrain.shape[0] * 0.3)]\n",
    "testTarget = dataset['val'][:-int(len(dataset['val']) * 0.3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem com SVM\n",
    "A abordagem com o SVM foi escolhida, pois ele apresenta bons resultados e por experiências anteriores com o algoritmo.\n",
    "\n",
    "A implementação utilizada disponível em [scikit-learn](http://scikit-learn.org/stable/index.html) uma biblioteca em Python amplamente utilizada para tarefas de *Machine Learning*.\n",
    "\n",
    "Parâmetros:\n",
    "* Kernel = 'rbf', recomendado por algumas referências [1] como uma boa escolha de primeira abordagem. O \"poly\" teve um desempenho muito ruim e o \"linear\" apresentou um score alto.\n",
    "* Gamma = 0, apresentou bons resultados quando \"rbf\"\n",
    "* C = 30, apresentou bons resultados quando \"rbf\", quando 100 aproximou de 98%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svm = SVC(kernel=\"linear\") # ~ 99 \n",
    "#svm = SVC(kernel=\"poly\") # ~ 19 \n",
    "#svm = SVC(kernel='rbf',gamma=0.001, C=100.) # ~ 98\n",
    "svm = SVC(kernel='rbf',gamma=0.001, C=30.) # ~87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Original number of features : %d\" % train.shape[1])\n",
    "svm.fit(train, trainTarget)\n",
    "predict = svm.predict(test)\n",
    "print(\"Score: \",svm.score(test, testTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (pd.crosstab(testTarget, predict, rownames=['Real'], colnames=['Classificado'], margins=True))\n",
    "#print(confusion_matrix(testTarget, predict, labels = [1, 2, 3,4,5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balancedDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação\n",
    "\n",
    "Para validar o classificador foi utilizado um dataset sem labels, o dataset é composto das seguintes colunas:\n",
    "\n",
    "1. id - Usuário no Twitter\n",
    "2. txt - O conteúdo do tweet\n",
    "3. cit - Cidade e País do usuário\n",
    "4. data - Data e hora do tweet\n",
    "5. dia - Dia da semana\n",
    "\n",
    "Total: 4202 amostras x 5 colunas.\n",
    "\n",
    "O dataset passou pelo mesmo pré-processamento de emoji que o dataset utilizado para treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetValida = pd.read_csv(\"demojized_input-emoji.csv\", sep=\"|\",quoting=csv.QUOTE_ALL)\n",
    "\n",
    "### Colocando espaços entre os emojis\n",
    "sentencesEmojis = [str(sentence).lower().replace(\":\",\" \") for sentence in datasetValida[\"txt\"]]\n",
    "datasetValida[\"txt\"] = sentencesEmojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = datasetValida[\"txt\"]\n",
    "validaCounts = count_vect.transform(tweets)\n",
    "validaTfidf = tfidf_transformer.transform(validaCounts)\n",
    "\n",
    "validaPredict = svm.predict(validaTfidf)\n",
    "\n",
    "for doc, category in zip(tweets, validaPredict):\n",
    "    #print(doc, \" \", category)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valência e Intensidade\n",
    "A valência foi predita pelo modelo e a intensidade adotada como padrão foi a 4 (neutra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetValida[\"val\"] = pd.Series(validaPredict, index=datasetValida.index)\n",
    "\n",
    "#Todas as intensidades como 4\n",
    "intensity = [4]*len(datasetValida)\n",
    "datasetValida[\"int\"] = pd.Series(intensity, index=datasetValida.index)\n",
    "\n",
    "datasetValida.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de dados para saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasetValida[\"txt\"] = sentencesEmojis\n",
    "datasetValida.head()\n",
    "\n",
    "formato = '%d/%m/%Y %H:%M'\n",
    "dateTimeConverted = [datetime.strptime(dt, formato).strftime('%m/%d/%y %H:%M') for dt in datasetValida[\"data\"]]\n",
    "datasetValida[\"data\"] = dateTimeConverted\n",
    "sentencesValida = [str(sent) for sent in datasetValida[\"txt\"]]\n",
    "datasetValida[\"txt\"] = sentencesValida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Histograma das classificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='val', data=datasetValida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetValida.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasetValida.to_csv('output-classificador-com-demoji-rbf-85.csv', \",\", index=False)\n",
    "print(\"Arquivo gerado: output-classificador-com-dmoji-85.csv \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "Foi possível obter uma classificação inicial válida e com bons resultados (em muitos casos \"overfitting\") a partir do SVM. Avaliando os diferentes parâmetros selecionados, obtivemos os seguintes resultados:\n",
    "\n",
    "* Kernel Linear - 99% e se apresentando um classificador \"otimista\"\n",
    "* Kernel Polynomial - 19% \n",
    "* Kernel Radial Basis Function (gamma=0.001, C=100) - 98% e se apresentando um classificador \"pessimista\"\n",
    "* Kernel Radial Basis Function (gamma=0.001, c=30) - 87% e classificou apenas as classes 3,4 e 5 (também \"pessimista\")\n",
    "\n",
    "Na matriz de confusão do Kernel Radial Basis Function (gamma=0.001, c=30), o modelo classificou muito bem as classes, errando mais na classe de valência 3, indicando uma forte tendência em classificar os tweets como negativos, o que chamamos aqui de “pessimista”. Outro interessante comportamento do classificador com tais parâmetros, é que classificou apenas as classes 3,4 e 5, o que podemos trocar por Negativo, Neutro e Positivo.\n",
    "\n",
    "Apesar das classificações serem dadas como \"pessimistas\" no caso do RBF, as classificações se mostraram, de certa forma, coerentes com o conteúdo dos tweets.\n",
    "\n",
    "Entre as dificuldades e fraquezas, podem ser citadas o processo de extração de *features*, nesse caso, para se aproximar do objetivo da ferramenta proposta no projeto, devemos estudar e adicionar outras *features* a serem consideradas no classificador, tais como final de semana, madrugada, entre outras que julgarmos relacionadas. \n",
    "No decorrer da implementação, encontramos algumas dificuldades em considerar os emojis, mas ao final da implementação, eles foram adicionados ao BoW e ao classificador.\n",
    "\n",
    "Uma questão importante a ser considerada, é de que os tweets possuem muitas gírias, siglas e erros ortográficos. Dessa forma, acreditamos que, como próximos passos, seja uma avaliação mais profunda no impacto disso no classificador e a resolução de tal problema.\n",
    "Outro ponto importante, como trabalho futuro, é a classificação de intensidade. Na implementação apresentada, apenas a classificação da Valência foi considerada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[1] NLM. Faceli K, Lorena AC, Gama J, Carvalho ACP de LF de. **Inteligência artificial: uma abordagem de aprendizado de máquina**. Rio de Janeiro: LTC, 2011.\n",
    "\n",
    "[2] [Aprendizado de Máquina com Python](https://iascblog.wordpress.com/2017/03/17/aprendizado-de-maquina-supervisionado-com-python/)\n",
    "\n",
    "[3] [Código de Exemplo](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n",
    "\n",
    "[4] [Cross Validation](http://juliocesarbatista.com/post/Cross-validation-testando-o-desempenho-de-um-classificador/)\n",
    "\n",
    "[5] [Scikit-learn - Trabalhando com Textos](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
