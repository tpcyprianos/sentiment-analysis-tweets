{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# TRatando hora\n",
    "datasetInFrame[\"dateDate\"] = datasetInFrame[\"date\"].apply(lambda x: datetime.strptime(x,'%m/%d/%Y').date())\n",
    "#datasetInFrame[\"timeTime\"] = datasetInFrame[\"time\"].apply(lambda x: x)\n",
    "#datasetInFrame[\"timeTime\"] = datasetInFrame[\"time\"].apply(lambda x: x.replace(\":\",\"\"))\n",
    "\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n",
    "\n",
    "datasetInFrame[\"dateInt\"] = datasetInFrame[\"dateDate\"].apply(lambda x: to_integer(x))\n",
    "\n",
    "#format = '%H:%M:%S'\n",
    "#print(datetime.strptime(\"0:17:49\", format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## Separando IDs\n",
    "usersName = datasetInFrame[\"id\"]\n",
    "del datasetInFrame[\"id\"]\n",
    "#datasetInFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "#format = '%m/%d/%Y %H:%M:%S'\n",
    "#datetime.strptime(\"10/27/2017 0:17:49\", format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Usuários\n",
    "users = []\n",
    "for i in datasetInFrame['id'].unique():\n",
    "    users.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Quantidade de amostras por valência\n",
    "for i in datasetInFrame['val'].unique():\n",
    "    x= datasetInFrame[datasetInFrame['val'] == i]\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"portuguese\"))\n",
    "#words = ' '.join([w for w in words if not w in stops])\n",
    "\n",
    "sentencesWithoutStopWords = []\n",
    "for i,row in dataset.iterrows():\n",
    "    sentence =  ' '.join([w for w in row[\"txt\"] if not w in stops])\n",
    "#    for word in row[\"txt\"]:\n",
    "#        if not word in stop_words:\n",
    "#            sentence = sentence + \" \"+ str(word)\n",
    "    sentencesWithoutStopWords.append(sentence.strip())\n",
    "dataset[\"txt\"]=sentencesWithoutStopWords\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_new = [\"Gente que NOJO dessa comida da escola, ainda bem que eu trouxe meu strogonoff\",\n",
    "            \"Ou reforma trabalhista?\",\n",
    "            \"Tô dormindo desde as 18hrs, dei um bolo bonito no Igor, tô me sentindo muito mal por isso\",\n",
    "            \"Nunca ganhei flores mas nossa é ilusão acha q ganha qlqr fita faz a pessoa ser saudável ctg ne\",\n",
    "            \"Toda vez q sento no banco preferencial penso q n surja num p mi tira daki amem\",\n",
    "            \"feliz e cansada\",\n",
    "            \"mano serio nao acredito q me planejei todinha financeiramente pq meu vale supostamente ia cair dia 20 e agora descubro q é dia 30\"]\n",
    "\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = svm.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(doc, \" \", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valida = [\"Alguém boto fogo na Marechal ta cherando maconha o metrô inteiro aaaaa\",\n",
    "          \"Prestígio 1 Real\",\n",
    "          \"Chega em casa e come arroz com linguisa\"]\n",
    "\n",
    "sentences = [str(sentence).lower().replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace('\"',\"\").replace(\"?\",\"\") for sentence in valida]\n",
    "valida = sentences\n",
    "\n",
    "sentencesWithTokens = [nltk.word_tokenize(sentence.lower()) for sentence in valida]\n",
    "valida = sentencesWithTokens\n",
    "\n",
    "sentencesWithout = []\n",
    "for i in valida:\n",
    "    sentence =  ' '.join([w for w in i if not w in stops])\n",
    "#    for word in row[\"txt\"]:\n",
    "#        if not word in stop_words:\n",
    "#            sentence = sentence + \" \"+ str(word)\n",
    "    sentencesWithout.append(sentence.strip())\n",
    "valida=sentencesWithout\n",
    "sentencesWithout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "# split the data into train and test\n",
    "train, test, trainTarget, testTarget = cross_validation.train_test_split(\n",
    "    balancedDataset['txt'], balancedDataset['val'], test_size = 0.3, random_state = 10)\n",
    "\n",
    "#fit_classifier(train, trainTarget)\n",
    "#predicted = svm.predict(test)\n",
    "# 0 = alt.atheism, 1 = comp.graphics, 2 = sci.med, 3 = soc.religion.christian\n",
    "print(confusion_matrix(testTarget, predict, labels = [1, 2, 3,4,5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "rfecv = RFECV(estimator=svm, step=1, cv=5, scoring='accuracy')\n",
    "X_rfecv=rfecv.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train classifier\n",
    "clf = MultinomialNB(alpha=0.5).fit(train, trainTarget)\n",
    "\n",
    "# test clf on test data\n",
    "X_test_rfecv = rfecv.transform(X_test_tfidf)\n",
    "\n",
    "predicted = clf.predict(X_test_rfecv) \n",
    "\n",
    "print(\"\\nAccuracy Training: \" + str(np.mean(predicted == labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# train classifier\n",
    "clf = svm.fit(X_rfecv, labels_train)\n",
    "\n",
    "# test clf on test data\n",
    "X_test_rfecv = rfecv.transform(X_test_tfidf)\n",
    "\n",
    "predicted = clf.predict(X_test_rfecv) \n",
    "\n",
    "print(\"\\nAccuracy Training: \" + str(np.mean(predicted == labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"confusion matrix\")\n",
    "# split the data into train and test\n",
    "train_data, test_data, train_labels, test_labels = cross_validation.train_test_split(\n",
    "    dataset.data, dataset.target, test_size = 0.1, random_state = 10)\n",
    "\n",
    "fit_classifier(train_data, train_labels)\n",
    "predicted = predict(test_data)\n",
    "# 0 = alt.atheism, 1 = comp.graphics, 2 = sci.med, 3 = soc.religion.christian\n",
    "print(confusion_matrix(test_labels, predicted, labels = [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todos os registros\n",
    "classifier.fit(sentencesWithoutStopWords, datasetInFrame[\"val\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(test_data):\n",
    "\ttest_counts = count_vector.transform(test_data)\n",
    "\ttest_tfidf = tfidf.transform(test_counts)\n",
    "return classifier.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "wordcloud(names(count_vect.vocabulary_), v, min.freq=10, scale=c(10,.3))\n",
    "\n",
    "d = path.dirname(__file__)\n",
    "\n",
    "# Read the whole text.\n",
    "text = open(path.join(d, 'constitution.txt')).read()\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Funcionando lindamente\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(X_tfidf.todense())\n",
    "data2D = pca.transform(X_tfidf.todense())\n",
    "balancedDataset['PCA-1'] = pd.Series(np.zeros(len(balancedDataset)), index=balancedDataset.index)\n",
    "balancedDataset['PCA-2'] = pd.Series(np.zeros(len(balancedDataset)), index=balancedDataset.index)\n",
    "balancedDataset.loc[:,'PCA-1'] = data2D[:,0]\n",
    "balancedDataset.loc[:,'PCA-2'] = data2D[:,1]\n",
    "sns.lmplot(x=\"PCA-1\", y=\"PCA-2\", hue=\"val\", data=balancedDataset, fit_reg=False)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5).fit(X_tfidf.todense())\n",
    "centers2D = pca.transform(kmeans.cluster_centers_)\n",
    "plt.hold(True)\n",
    "plt.scatter(centers2D[:,0], centers2D[:,1], \n",
    "            marker='x', s=200, linewidths=3, c='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
